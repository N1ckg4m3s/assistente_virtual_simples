{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/N1ckg4m3s/assistente_virtual_simples/blob/main/Assistente_ViniTabacaria_v2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Caminho_Modelo = '/content/ChatBotModels'\n",
        "\n",
        "Caminho_DataSet = '/content/DataSetParts/'\n",
        "\n",
        "Caminho_Arquivos_DataSet=[]"
      ],
      "metadata": {
        "id": "QWA-TYkW09CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TODAS AS IMPORTAÇÕES E CARREGAMENTOS**"
      ],
      "metadata": {
        "id": "lw5InEBwx7eW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importando bibliotecas"
      ],
      "metadata": {
        "id": "A-AVxYVgy1Ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "import gc\n",
        "import json\n",
        "\n",
        "\n",
        "from transformers import (GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, TrainerCallback,\n",
        "                          get_linear_schedule_with_warmup, GPT2Config)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import AdamW\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n"
      ],
      "metadata": {
        "id": "DJGLIz1Ax7E-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b031e23-9026-4ad3-fb74-520284957b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importando DataSet"
      ],
      "metadata": {
        "id": "WRyiVbIUy2iS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## baixar o Dataset\n",
        "!git clone https://github.com/N1ckg4m3s/Chat_bot_ViniTabacaria.git"
      ],
      "metadata": {
        "id": "mQ4aV4xey6Cr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3adf75b0-6995-47ca-82e8-dfc5460b7490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Chat_bot_ViniTabacaria'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 27 (delta 3), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (27/27), 60.11 KiB | 3.00 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importando dos pesos do modelo"
      ],
      "metadata": {
        "id": "iypjecCCy4bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "!ln -s /content/drive/MyDrive/ChatBotModels /content/ChatBotModels"
      ],
      "metadata": {
        "id": "zMcgKA-Ty6RE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9881f1d3-50e7-4bf2-dbcb-6e598d4bc9e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FUNÇÕES E CLASSES AUXILIARES**"
      ],
      "metadata": {
        "id": "eym1uXPXyCYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(inputs, outputs, tokenizer, max_length=1024):\n",
        "    # Tokenizando as entradas e saídas, com max_length garantido para ambos\n",
        "    input_encodings = tokenizer(inputs, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    output_encodings = tokenizer(outputs, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "\n",
        "    # Aqui, truncamos ou ajustamos os labels para ter o mesmo comprimento que as entradas\n",
        "    output_encodings['labels'] = output_encodings['input_ids'].clone()\n",
        "\n",
        "    # Ajustar os labels para que o comprimento seja igual ao das entradas\n",
        "    output_encodings['labels'] = output_encodings['labels'][:, :input_encodings['input_ids'].size(1)]\n",
        "\n",
        "    return input_encodings, output_encodings\n"
      ],
      "metadata": {
        "id": "TYaIye2BySRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits_here, labels_here = eval_pred\n",
        "\n",
        "    # Garantir que logits e labels sejam tensores\n",
        "    logits_here = torch.tensor(logits_here) if not isinstance(logits_here, torch.Tensor) else logits_here\n",
        "    labels_here = torch.tensor(labels_here) if not isinstance(labels_here, torch.Tensor) else labels_here\n",
        "\n",
        "    # Calcula a perda\n",
        "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "    loss = loss_fct(logits_here.view(-1, logits_here.size(-1)), labels_here.view(-1))\n",
        "    perplexity = torch.exp(loss)\n",
        "\n",
        "    # Transformar logits em previsões de classe\n",
        "    preds = torch.argmax(logits_here, dim=-1)\n",
        "    preds_flat = preds.view(-1).cpu().numpy()\n",
        "    labels_flat = labels_here.view(-1).cpu().numpy()\n",
        "\n",
        "    # Excluir valores ignorados (-100)\n",
        "    valid_indices = labels_flat != -100\n",
        "    preds_flat = preds_flat[valid_indices]\n",
        "    labels_flat = labels_flat[valid_indices]\n",
        "\n",
        "    # Métricas de precisão\n",
        "    precision = precision_score(labels_flat, preds_flat, average='macro', zero_division=0)\n",
        "\n",
        "    # Criar uma métrica combinada (exemplo: média ponderada)\n",
        "    combined_metric = (1000 * precision) + (1000 / (1 + perplexity.item()))\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss.item(),\n",
        "        \"perplexity\": perplexity.item(),\n",
        "        \"precision\": precision,\n",
        "        \"combined_metric\": combined_metric\n",
        "    }"
      ],
      "metadata": {
        "id": "5NCGsJh41Mo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, input_encodings, output_encodings):\n",
        "        # Verifique se o número de exemplos é o mesmo\n",
        "        assert len(input_encodings['input_ids']) == len(output_encodings['labels']), \\\n",
        "            f\"Mismatch in input and output sizes: {len(input_encodings['input_ids'])} != {len(output_encodings['labels'])}\"\n",
        "\n",
        "        self.input_ids = input_encodings['input_ids']\n",
        "        self.attention_mask = input_encodings['attention_mask']\n",
        "        self.labels = output_encodings['labels']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }"
      ],
      "metadata": {
        "id": "Qkp-EWmv0flO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStoppingCallback(TrainerCallback):\n",
        "    def __init__(self, patience=3):\n",
        "        self.patience = patience\n",
        "        self.best_metric = float('-inf')\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        # A métrica já estará armazenada em state.best_metric\n",
        "        combined_metric = state.best_metric\n",
        "        if combined_metric > self.best_metric:\n",
        "            self.best_metric = combined_metric\n",
        "            self.stopped_epoch = 0  # Reset se houver melhoria\n",
        "        else:\n",
        "            self.stopped_epoch += 1\n",
        "\n",
        "        if self.stopped_epoch >= self.patience:\n",
        "            print(f\"Early stopping triggered at epoch {state.epoch}\")\n",
        "            control.should_early_stop = True\n",
        "        return control"
      ],
      "metadata": {
        "id": "BX7RyMDyT4p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CARREGANDO O TOKENIZADOR**"
      ],
      "metadata": {
        "id": "FksMlVde0xho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(Caminho_Modelo)\n",
        "except:\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "VUTBV9c200lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CARREGANDO O DATASET**"
      ],
      "metadata": {
        "id": "Tp2SwBjDx9p9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregando dados do DataSet\n"
      ],
      "metadata": {
        "id": "EVgo-ODFz0sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Caminho_Arquivos_DataSet=[]\n",
        "# Carregar o arquivo JSON\n",
        "with open(\"/content/Chat_bot_ViniTabacaria/DataSet/modelos_gerados.json\", \"r\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "if not os.path.exists(Caminho_DataSet):\n",
        "    os.makedirs(Caminho_DataSet)\n",
        "\n",
        "# Dividir os dados em 10 partes\n",
        "num_files = 10\n",
        "chunk_size = len(data) // num_files\n",
        "chunks = [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n",
        "\n",
        "# Se houver sobra de elementos (quando len(data) não for múltiplo de 10), adicionar os extras ao último arquivo\n",
        "if len(data) % num_files != 0:\n",
        "    chunks[-1].extend(data[num_files * chunk_size:])\n",
        "\n",
        "# Salvar os dados divididos em 10 arquivos\n",
        "for i, chunk in enumerate(chunks):\n",
        "    output_file = f\"{Caminho_DataSet}part_{i+1}.json\"  # Caminho correto do arquivo\n",
        "    with open(output_file, \"w\") as file:\n",
        "        json.dump(chunk, file, indent=4)\n",
        "\n",
        "    print(f\"Arquivo {output_file} salvo com sucesso.\")\n",
        "\n",
        "    Caminho_Arquivos_DataSet.append(output_file)\n",
        "\n",
        "# Excluir as variáveis para liberar memória\n",
        "del data\n",
        "del chunks\n",
        "\n",
        "# Forçar a coleta de lixo para liberar memória\n",
        "gc.collect()\n",
        "\n",
        "print(\"Arquivos divididos, salvos e memória liberada com sucesso.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tpv_tSsqQw0v",
        "outputId": "d9753ca7-f5f8-4f48-9067-c138960aeb4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo /content/DataSetParts/part_1.json salvo com sucesso.\n",
            "Arquivo /content/DataSetParts/part_2.json salvo com sucesso.\n",
            "Arquivo /content/DataSetParts/part_3.json salvo com sucesso.\n",
            "Arquivo /content/DataSetParts/part_4.json salvo com sucesso.\n",
            "Arquivo /content/DataSetParts/part_5.json salvo com sucesso.\n",
            "Arquivo /content/DataSetParts/part_6.json salvo com sucesso.\n",
            "Arquivo /content/DataSetParts/part_7.json salvo com sucesso.\n",
            "Arquivo /content/DataSetParts/part_8.json salvo com sucesso.\n",
            "Arquivo /content/DataSetParts/part_9.json salvo com sucesso.\n",
            "Arquivo /content/DataSetParts/part_10.json salvo com sucesso.\n",
            "Arquivo /content/DataSetParts/part_11.json salvo com sucesso.\n",
            "Arquivos divididos, salvos e memória liberada com sucesso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separando parte para Teste e Validação"
      ],
      "metadata": {
        "id": "qODmcsIVz3Hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GetTrain_Val_Data(Caminho_Arquivos_DataSet, tokenizer):\n",
        "    # Carregar os dados do arquivo\n",
        "    with open(Caminho_Arquivos_DataSet, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Separar dados de treino e validação\n",
        "    inputs_train, inputs_val, outputs_train, outputs_val = train_test_split(\n",
        "        [item[\"input\"] for item in data],\n",
        "        [item[\"output\"] for item in data],\n",
        "        test_size=0.2,\n",
        "        shuffle=True,\n",
        "        random_state=85\n",
        "    )\n",
        "\n",
        "    # Criar datasets de treino e validação\n",
        "    input_encode_train, output_encode_train = tokenize_data(inputs_train, outputs_train, tokenizer)\n",
        "    input_encode_val, output_encode_val = tokenize_data(inputs_val, outputs_val, tokenizer)\n",
        "\n",
        "    # Criar os datasets personalizados para treino e validação\n",
        "    train_dataset = CustomDataset(input_encode_train, output_encode_train)\n",
        "    val_dataset = CustomDataset(input_encode_val, output_encode_val)\n",
        "\n",
        "    # Limpar as variáveis para liberar memória\n",
        "    del data, inputs_train, inputs_val, outputs_train, outputs_val, input_encode_train, output_encode_train, input_encode_val, output_encode_val\n",
        "\n",
        "    # Forçar a coleta de lixo para liberar memória\n",
        "    gc.collect()\n",
        "\n",
        "    # Retornar os datasets\n",
        "    return train_dataset, val_dataset"
      ],
      "metadata": {
        "id": "-xhWCOqYz5dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONFIG. MODELO**\n"
      ],
      "metadata": {
        "id": "WqopqJ6E18bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "\n",
        "config = GPT2Config(\n",
        "    n_layer=6,         # Reduz para 6 camadas\n",
        "    n_embd=256,        # Tamanho do embedding reduzido\n",
        "    n_head=4,          # Reduz para 4 cabeças de atenção\n",
        "    n_positions=512,   # Limita o número de posições\n",
        "    n_ctx=512          # Limita o contexto\n",
        ")"
      ],
      "metadata": {
        "id": "fQp3qfFE2b_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    model = GPT2LMHeadModel.from_pretrained(Caminho_Modelo, config=config)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(Caminho_Modelo)\n",
        "    print(\"Modelo e tokenizador carregados com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao carregar o modelo e tokenizador: {e}\")\n",
        "    model = GPT2LMHeadModel(config)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Preparar o modelo para treinamento\n",
        "model.train()"
      ],
      "metadata": {
        "id": "jWWxnfFP2m-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd36e458-734e-4b7a-c8ef-788bf7c227b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo e tokenizador carregados com sucesso!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 256)\n",
              "    (wpe): Embedding(512, 256)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x GPT2Block(\n",
              "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=768, nx=256)\n",
              "          (c_proj): Conv1D(nf=256, nx=256)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=1024, nx=256)\n",
              "          (c_proj): Conv1D(nf=256, nx=1024)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=256, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./model_output\",\n",
        "    num_train_epochs=num_epochs,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,\n",
        "    max_grad_norm=1.0,\n",
        "    metric_for_best_model=\"combined_metric\",\n",
        "    greater_is_better=False,\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "CyEu1FVN1-3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CARREGAR MODELO/TOKENIZADOR**"
      ],
      "metadata": {
        "id": "Pd27chGAykOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "for Caminho in Caminho_Arquivos_DataSet:\n",
        "    try:\n",
        "        model = GPT2LMHeadModel.from_pretrained(Caminho_Modelo, config=config)\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(Caminho_Modelo)\n",
        "    except Exception as e:\n",
        "        model = GPT2LMHeadModel(config)\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # Preparar o modelo para treinamento\n",
        "    model.train()\n",
        "\n",
        "    train_dataset, val_dataset = GetTrain_Val_Data(Caminho, tokenizer)\n",
        "\n",
        "    # Criar o otimizador e o agendador de taxa de aprendizado\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "    total_steps = len(train_dataset) * num_epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(total_steps * 0.1),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(patience=3)]\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    model.save_pretrained(Caminho_Modelo=='' and '/content/ChatBotModels' or Caminho_Modelo)\n",
        "    tokenizer.save_pretrained(Caminho_Modelo=='' and '/content/ChatBotModels' or Caminho_Modelo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "collapsed": true,
        "id": "VUn-4UsTSQ2B",
        "outputId": "85dc0e41-da76-45d5-c144-382d76a23434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='17' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 17/800 00:21 < 18:24, 0.71 it/s, Epoch 1/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'>' not supported between instances of 'NoneType' and 'float'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-5f47c5d98032>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEarlyStoppingCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     )\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCaminho_Modelo\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m''\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'/content/ChatBotModels'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCaminho_Modelo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2615\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2617\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2618\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_epoch_end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_step_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    520\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-8bd1cb6aea60>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# A métrica já estará armazenada em state.best_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mcombined_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcombined_metric\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_metric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# Reset se houver melhoria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'float'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('/content/ChatBotModels')\n",
        "tokenizer.save_pretrained('/content/ChatBotModels')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsAncN-3yS6y",
        "outputId": "4a8dfcc5-590c-40cf-fa46-b13284813b56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/ChatBotModels/tokenizer_config.json',\n",
              " '/content/ChatBotModels/special_tokens_map.json',\n",
              " '/content/ChatBotModels/vocab.json',\n",
              " '/content/ChatBotModels/merges.txt',\n",
              " '/content/ChatBotModels/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VALIDAR MODELO**"
      ],
      "metadata": {
        "id": "pwND2wuq229X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    model = GPT2LMHeadModel.from_pretrained(Caminho_Modelo, config=config)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(Caminho_Modelo)\n",
        "except Exception as e:\n",
        "    model = GPT2LMHeadModel(config)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model.eval()\n",
        "\n",
        "# Função para gerar resposta e calcular o loss\n",
        "def gerar_resposta_com_loss(texto, max_length=60):\n",
        "    # Tokenizar o texto de entrada\n",
        "    inputs = tokenizer.encode(texto, return_tensors='pt')\n",
        "\n",
        "    # Usar o mesmo texto como 'labels' para calcular a perda\n",
        "    labels = inputs.clone()\n",
        "\n",
        "    # Gerar a saída com o modelo\n",
        "    outputs = model(input_ids=inputs, labels=labels)\n",
        "\n",
        "    # A perda é armazenada no atributo 'loss'\n",
        "    loss = outputs.loss\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Gerar a sequência de texto com o método generate\n",
        "    outputs_generate = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,            # Limitar o comprimento da resposta\n",
        "        num_return_sequences=1,           # Apenas uma resposta\n",
        "        num_beams=5,                      # Beam search com 5 opções\n",
        "        no_repeat_ngram_size=4,           # Evitar repetições de 4-gramas\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decodificar a resposta gerada para texto\n",
        "    resposta = tokenizer.decode(outputs_generate[0], skip_special_tokens=True)\n",
        "\n",
        "    return resposta, loss.item()"
      ],
      "metadata": {
        "id": "hsidmvsu2-7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Testar com um exemplo personalizado\n",
        "texto_entrada = \"Quero uma ziggy uva\"\n",
        "resposta_gerada, loss_gerado = gerar_resposta_com_loss(texto_entrada, max_length=100)\n",
        "\n",
        "# Mostrar a resposta gerada e a perda calculada\n",
        "print(\"Resposta gerada:\", resposta_gerada)\n",
        "print(\"Loss calculado:\", loss_gerado)\n"
      ],
      "metadata": {
        "id": "-1KknsMJ25oX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31bdd73c-6284-4e11-e086-261647934960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta gerada: Quero uma ziggy uvaduto],Marca=,sabor=uva)u\n",
            "Loss calculado: 15.285144805908203\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}